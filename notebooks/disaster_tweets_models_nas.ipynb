{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center; color: #BD6C37;\"> <i> Ecole Polytechnique de Thi√®s <br>  D√©partement G√©nie Informatique et T√©l√©communications </i> </h4>\n",
    "<h1 style=\"text-align: center\"> Principes MLOps </h1>\n",
    "<h5 style=\"text-align: center\">DIC3-GIT, 2023-2024</h5>\n",
    "<h5 style=\"text-align: center\">Mme Mously DIAW</h5>\n",
    "<h1 style=\"text-align: center; color:#90edaa\">Projet mati√®re : Natural Language Processing with Disaster Tweets</h1>\n",
    "<h5 style=\"text-align: center\"> Par Kikia DIA, Mouhamadou Naby DIA, Ndeye Awa SALANE </h5>\n",
    "<h3 style=\"text-align: center; color:#9000aa; text-decoration:underline\"> II. Models Exploration </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a> <br>\n",
    "### Sommaire\n",
    "#### [Introduction](#1)\n",
    "1. [Features Selection](#3)\n",
    "1. [Vectorisation](#2)\n",
    "1. [Models building](#4)\n",
    "1. [Pipeline](#8)\n",
    "#### [Conclusion](#5)\n",
    "* <i>[References](#6)</i>\n",
    "* <i>[Authors](#7)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    "#### Introduction [‚èÆÔ∏è]()[üëÜüèΩ](#0)[‚è≠Ô∏è](#2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter le r√©pertoire parent pour les imports de module\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import pycaret\n",
    "from pycaret.classification import *\n",
    "from pyngrok import ngrok\n",
    "import ppscore as pps\n",
    "import seaborn as sns\n",
    "from settings.params import MODEL_PARAMS, SEED\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from src.data.make_dataset import get_dataset\n",
    "from xgboost import XGBClassifier\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_dataset(raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7591, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may forgive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>receive wildfire evacuation order</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>get send photo ruby smoke pour school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN                 deed reason earthquake may forgive   \n",
       "1   4     NaN      NaN                         forest fire near la canada   \n",
       "2   5     NaN      NaN  resident ask shelter place notify officer evac...   \n",
       "3   6     NaN      NaN                  receive wildfire evacuation order   \n",
       "4   7     NaN      NaN              get send photo ruby smoke pour school   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7591, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    "#### 1. Feature Selection [‚èÆÔ∏è](#1)[üëÜüèΩ](#0)[‚è≠Ô∏è](#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      deed reason earthquake may forgive\n",
       "1                              forest fire near la canada\n",
       "2       resident ask shelter place notify officer evac...\n",
       "3                       receive wildfire evacuation order\n",
       "4                   get send photo ruby smoke pour school\n",
       "                              ...                        \n",
       "7586     two giant crane hold bridge collapse nearby home\n",
       "7587    aria control wild fire even northern part stat...\n",
       "7588                                              volcano\n",
       "7589    police investigate bike collide car little bik...\n",
       "7590                     late home raze northern wildfire\n",
       "Name: text, Length: 7591, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate 'text' and 'Keywords' if 'Keywords' is not NaN\n",
    "# train['combined_text'] = train.apply(\n",
    "#     # lambda row: f\"{row['text']} {row['keyword'] or ''} \".strip(),\n",
    "#     lambda row: f\"{row['text']} {row['keyword'] or ''} {row['location'] or ''}\".strip(),\n",
    "\n",
    "#     axis=1\n",
    "# )\n",
    "# X = train['combined_text']\n",
    "X = train[\"text\"]\n",
    "y = train['target']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CHI2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Sample data - Replace this with your tweets and labels\n",
    "# data = train\n",
    "# tweets = data[\"text\"]\n",
    "# labels = data[\"target\"]\n",
    "\n",
    "# # Convert text data to numerical features\n",
    "# vectorizer = CountVectorizer(stop_words='english')\n",
    "# X = vectorizer.fit_transform(tweets)\n",
    "\n",
    "\n",
    "# # Apply Chi-Square test\n",
    "# chi2_scores, p_values = chi2(X, y)\n",
    "\n",
    "# # Create a DataFrame to view feature names and their scores\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "# chi2_df = pd.DataFrame({'Feature': feature_names, 'Chi2 Score': chi2_scores})\n",
    "\n",
    "# # Sort by Chi2 Score in descending order\n",
    "# chi2_df = chi2_df.sort_values(by='Chi2 Score', ascending=False)\n",
    "\n",
    "# # Display the top features\n",
    "# print(chi2_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    "#### 2. Vectorisation [‚èÆÔ∏è](#2)[üëÜüèΩ](#0)[‚è≠Ô∏è](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_vect = cv.fit_transform(X)\n",
    "# X_vect = X_vect.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vect = tfidf.fit_transform(X)\n",
    "X_vect = X_vect.toarray()\n",
    "X_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(X_vect[2].T.todense(),\n",
    "#     \tindex=tfidf.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "# df = df.sort_values('TF-IDF', ascending=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the text data\n",
    "# def preprocess_text(text):\n",
    "#     # Tokenize the text\n",
    "#     tokens = word_tokenize(text.lower())\n",
    "#     return tokens\n",
    "\n",
    "# # Apply preprocessing to the text data\n",
    "# X_tokenized = X.apply(preprocess_text)\n",
    "\n",
    "# # Train Word2Vec model\n",
    "# word2vec_model = Word2Vec(sentences=X_tokenized, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# # Transform text data into vectors by averaging the word vectors\n",
    "# def transform_text_to_vector(tokens, model):\n",
    "#     word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "#     if len(word_vectors) == 0:\n",
    "#         return np.zeros(model.vector_size)\n",
    "#     return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# X_vect = X_tokenized.apply(lambda tokens: transform_text_to_vector(tokens, word2vec_model))\n",
    "# X_vect = np.vstack(X_vect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"3\"></a> \n",
    "#### 3. Model building [‚èÆÔ∏è](#3)[üëÜüèΩ](#0)[‚è≠Ô∏è](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train / Validation split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect, X_val_vect, y_train, y_val = train_test_split(X_vect, y, test_size=0.15, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up MLflow tracking\n",
    "local_registry = \"sqlite:///mlruns.db\"\n",
    "mlflow.set_tracking_uri(local_registry)\n",
    "\n",
    "# Create an experiment\n",
    "exp_name = \"disaster_tweets_classification\"\n",
    "experiment = mlflow.get_experiment_by_name(exp_name)\n",
    "if not experiment:\n",
    "    experiment_id = mlflow.create_experiment(exp_name)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine features and labels into a DataFrame\n",
    "# df = pd.DataFrame(X_vect)\n",
    "# df['target'] = y\n",
    "\n",
    "# # Use PyCaret for classification\n",
    "# clf = setup(data=df, target='target', session_id=123)\n",
    "\n",
    "# # Compare different models\n",
    "# best_model = compare_models()\n",
    "\n",
    "# # Pull the results\n",
    "# results = pull()\n",
    "\n",
    "# # Print the results\n",
    "# print(results)\n",
    "\n",
    "# # Finalize the best model (optional)\n",
    "# final_model = finalize_model(best_model)\n",
    "\n",
    "# # Print the best model details\n",
    "# print(final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search for some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_with_grid_search(model, param_grid, X_train, y_train, X_val, y_val, model_name):\n",
    "    # Perform Grid Search\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                               cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model from Grid Search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name, experiment_id=experiment_id) as run:\n",
    "        # Train the best model\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "        # Calculate metrics for training set\n",
    "        train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "        train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "        train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "        # Calculate metrics for validation set\n",
    "        val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "        val_precision = precision_score(y_val, y_val_pred, average='weighted')\n",
    "        val_recall = recall_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_params(best_params)\n",
    "        mlflow.log_metric(\"train_f1_score\", train_f1)\n",
    "        mlflow.log_metric(\"train_precision\", train_precision)\n",
    "        mlflow.log_metric(\"train_recall\", train_recall)\n",
    "        mlflow.log_metric(\"val_f1_score\", val_f1)\n",
    "        mlflow.log_metric(\"val_precision\", val_precision)\n",
    "        mlflow.log_metric(\"val_recall\", val_recall)\n",
    "\n",
    "        # Log the best model\n",
    "        mlflow.sklearn.log_model(best_model, model_name, input_example=X_train[:30])\n",
    "\n",
    "        # Print metrics\n",
    "        print(f'{model_name} Training F1 Score: {train_f1}')\n",
    "        print(f'{model_name} Training Precision: {train_precision}')\n",
    "        print(f'{model_name} Training Recall: {train_recall}')\n",
    "        print(f'{model_name} Validation F1 Score: {val_f1}')\n",
    "        print(f'{model_name} Validation Precision: {val_precision}')\n",
    "        print(f'{model_name} Validation Recall: {val_recall}')\n",
    "\n",
    "        # Print best parameters\n",
    "        print(f'Best parameters for {model_name}: {best_params}')\n",
    "    return grid_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classification models with fixed random seed\n",
    "models = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=SEED),\n",
    "    'SVC': SVC(C=100, gamma='scale', kernel='linear', random_state=SEED),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=SEED),\n",
    "    'Logistic Regression': LogisticRegression(C=0.8)\n",
    "}\n",
    "\n",
    "# Parameter grids for each model\n",
    "param_grids = {\n",
    "    'RandomForestClassifier': [{\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }],\n",
    "    'SVC': [{\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "    }],\n",
    "    'Naive Bayes': [{\n",
    "        'alpha': [0.01, 0.1, 1, 10]\n",
    "    }],\n",
    "    'KNN': [{\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    }],\n",
    "    'XGBoost': [{\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.8, 1.0]\n",
    "    }],\n",
    "    'Logistic Regression': [{\n",
    "        'C': [0.01, 0.1, 0.8, 1, 10, 100],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': [100, 200, 500, 1000],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]  # Only used if penalty is 'elasticnet'\n",
    "    }]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Grid Search for RandomForestClassifier...\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming Grid Search for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     best_models[model_name] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_with_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_vect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_vect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m, in \u001b[0;36mtrain_and_evaluate_with_grid_search\u001b[1;34m(model, param_grid, X_train, y_train, X_val, y_val, model_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_evaluate_with_grid_search\u001b[39m(model, param_grid, X_train, y_train, X_val, y_val, model_name):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Perform Grid Search\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, \n\u001b[0;32m      4\u001b[0m                                cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Get the best model from Grid Search\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform Grid Search on each model\n",
    "best_models = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Performing Grid Search for {model_name}...\")\n",
    "    best_models[model_name] = train_and_evaluate_with_grid_search(model, param_grids[model_name], X_train_vect, y_train, X_val_vect, y_val, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train and evaluate models\n",
    "def train_and_evaluate(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    with mlflow.start_run(run_name=model_name, experiment_id=experiment_id) as run:\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        # Calculate metrics for training set\n",
    "        train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "        train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "        train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "        # Calculate metrics for validation set\n",
    "        val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "        val_precision = precision_score(y_val, y_val_pred, average='weighted')\n",
    "        val_recall = recall_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_params(model.get_params())\n",
    "        mlflow.log_metric(\"train_f1_score\", train_f1)\n",
    "        mlflow.log_metric(\"train_precision\", train_precision)\n",
    "        mlflow.log_metric(\"train_recall\", train_recall)\n",
    "        mlflow.log_metric(\"val_f1_score\", val_f1)\n",
    "        mlflow.log_metric(\"val_precision\", val_precision)\n",
    "        mlflow.log_metric(\"val_recall\", val_recall)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(model, model_name)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f'{model_name} Training F1 Score: {train_f1}')\n",
    "        print(f'{model_name} Training Precision: {train_precision}')\n",
    "        print(f'{model_name} Training Recall: {train_recall}')\n",
    "        print(f'{model_name} Validation F1 Score: {val_f1}')\n",
    "        print(f'{model_name} Validation Precision: {val_precision}')\n",
    "        print(f'{model_name} Validation Recall: {val_recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_and_evaluate(model, param_grids[model_name], X_train_vect, y_train, X_val_vect, y_val, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/06 20:23:18 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier Training F1 Score: 0.9816806422576758\n",
      "RandomForestClassifier Training Precision: 0.9818872281650376\n",
      "RandomForestClassifier Training Recall: 0.98171109733416\n",
      "RandomForestClassifier Validation F1 Score: 0.7823297680413072\n",
      "RandomForestClassifier Validation Precision: 0.7874803797384721\n",
      "RandomForestClassifier Validation Recall: 0.7857769973661106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/06 20:25:52 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Training F1 Score: 0.9764021832471756\n",
      "SVC Training Precision: 0.9765986596552716\n",
      "SVC Training Recall: 0.9764414135151891\n",
      "SVC Validation F1 Score: 0.7407608782093515\n",
      "SVC Validation Precision: 0.7405772973652628\n",
      "SVC Validation Recall: 0.7410008779631255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/06 20:25:56 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training F1 Score: 0.86870304428356\n",
      "Naive Bayes Training Precision: 0.8734729655368948\n",
      "Naive Bayes Training Recall: 0.870272783632982\n",
      "Naive Bayes Validation F1 Score: 0.7782802540920541\n",
      "Naive Bayes Validation Precision: 0.7846659395802227\n",
      "Naive Bayes Validation Recall: 0.7822651448639157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/06 20:26:08 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Training F1 Score: 0.8247413837493035\n",
      "KNN Training Precision: 0.8305193403409592\n",
      "KNN Training Recall: 0.8273403595784253\n",
      "KNN Validation F1 Score: 0.7555838177878885\n",
      "KNN Validation Precision: 0.7620752382663547\n",
      "KNN Validation Recall: 0.7603160667251976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/06 20:26:16 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Training F1 Score: 0.8436545450751911\n",
      "XGBoost Training Precision: 0.8572830449371619\n",
      "XGBoost Training Recall: 0.847334159950403\n",
      "XGBoost Validation F1 Score: 0.7707382424445831\n",
      "XGBoost Validation Precision: 0.7806826590989302\n",
      "XGBoost Validation Recall: 0.7761194029850746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/06 20:26:20 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training F1 Score: 0.8593260438056409\n",
      "Logistic Regression Training Precision: 0.8679312620431813\n",
      "Logistic Regression Training Recall: 0.8617482951022939\n",
      "Logistic Regression Validation F1 Score: 0.777894968203914\n",
      "Logistic Regression Validation Precision: 0.7914551062193333\n",
      "Logistic Regression Validation Recall: 0.7840210711150132\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    train_and_evaluate(model,  X_train_vect, y_train, X_val_vect, y_val, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voting & Stacking Classifiers with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VotingClassifier with the best models\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    # ('rf', models['RandomForestClassifier']),\n",
    "    # ('svc', models['SVC']),\n",
    "    ('nb', models['Naive Bayes']),\n",
    "    # ('knn', models['KNN']),\n",
    "    ('xgb', models['XGBoost']),\n",
    "    ('lr',  models['Logistic Regression']),\n",
    "], voting='hard')\n",
    "\n",
    "stacking_clf = StackingClassifier(estimators=[\n",
    "    # ('rf', models['RandomForestClassifier']),\n",
    "    # ('svc', models['SVC']),\n",
    "    ('nb', models['Naive Bayes']),\n",
    "    # ('knn', models['KNN']),\n",
    "    ('xgb', models['XGBoost']),\n",
    "        ('lr',  models['Logistic Regression']),\n",
    "\n",
    "], final_estimator=LogisticRegression(),\n",
    "    cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cceef64eaf4e729c2cec6e9363e814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble_VotingClassifier Training F1 Score: 0.863603770780156\n",
      "Ensemble_VotingClassifier Training Precision: 0.8722241400949332\n",
      "Ensemble_VotingClassifier Training Recall: 0.8659330440173589\n",
      "Ensemble_VotingClassifier Validation F1 Score: 0.7912193359751131\n",
      "Ensemble_VotingClassifier Validation Precision: 0.8028585655908783\n",
      "Ensemble_VotingClassifier Validation Recall: 0.7963125548726954\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_ensemble(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    with mlflow.start_run(run_name=model_name, experiment_id=experiment_id) as run:\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        # Calculate metrics for training set\n",
    "        train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "        train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "        train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "        # Calculate metrics for validation set\n",
    "        val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "        val_precision = precision_score(y_val, y_val_pred, average='weighted')\n",
    "        val_recall = recall_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_param(\"ensemble_method\", \"VotingClassifier\")\n",
    "        mlflow.log_metric(\"train_f1_score\", train_f1)\n",
    "        mlflow.log_metric(\"train_precision\", train_precision)\n",
    "        mlflow.log_metric(\"train_recall\", train_recall)\n",
    "        mlflow.log_metric(\"val_f1_score\", val_f1)\n",
    "        mlflow.log_metric(\"val_precision\", val_precision)\n",
    "        mlflow.log_metric(\"val_recall\", val_recall)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(model, model_name, input_example=X_train[:30])\n",
    "\n",
    "        # Print metrics\n",
    "        print(f'{model_name} Training F1 Score: {train_f1}')\n",
    "        print(f'{model_name} Training Precision: {train_precision}')\n",
    "        print(f'{model_name} Training Recall: {train_recall}')\n",
    "        print(f'{model_name} Validation F1 Score: {val_f1}')\n",
    "        print(f'{model_name} Validation Precision: {val_precision}')\n",
    "        print(f'{model_name} Validation Recall: {val_recall}')\n",
    "\n",
    "# Train and evaluate the ensemble model\n",
    "train_and_evaluate_ensemble(voting_clf, X_train_vect, y_train, X_val_vect, y_val, \"Ensemble_VotingClassifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43a6598e72c4dc0a11eae30d8b24f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble_StackingClassifier Training F1 Score: 0.8749792125648127\n",
      "Ensemble_StackingClassifier Training Precision: 0.8790034884796407\n",
      "Ensemble_StackingClassifier Training Recall: 0.8763174209547427\n",
      "Ensemble_StackingClassifier Validation F1 Score: 0.7945039994239871\n",
      "Ensemble_StackingClassifier Validation Precision: 0.7983152842613692\n",
      "Ensemble_StackingClassifier Validation Recall: 0.797190517998244\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_ensemble(stacking_clf, X_train_vect, y_train, X_val_vect, y_val, \"Ensemble_StackingClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Tracking UI: NgrokTunnel: \"https://5c31-41-82-155-235.ngrok-free.app\" -> \"http://localhost:5000\"\n"
     ]
    }
   ],
   "source": [
    "# Terminate open tunnels if they exist\n",
    "ngrok.kill()\n",
    "NGROK_AUTH_TOKEN = \"2iXzpfikPynAXlcC1qn3NDhkhMS_7BbPuGQ5Sc8R7aohgotz6\"\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "public_url = ngrok.connect(5000)\n",
    "print(\"MLflow Tracking UI:\", public_url)\n",
    "\n",
    "# Transition models to production\n",
    "client = MlflowClient()\n",
    "for model_name in models.keys():\n",
    "    try:\n",
    "        model_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "        if model_versions:\n",
    "            # Get the latest model version\n",
    "            latest_version = model_versions[-1]\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=latest_version.version,\n",
    "                stage=\"production\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error transitioning model {model_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"3\"></a> \n",
    "#### 4. Pipeline [‚èÆÔ∏è](#4)[üëÜüèΩ](#0)[‚è≠Ô∏è](#6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m idf_pip\u001b[38;5;241m=\u001b[39m\u001b[43mPipeline\u001b[49m(\n\u001b[0;32m      2\u001b[0m                 [\n\u001b[0;32m      3\u001b[0m                     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_idf\u001b[39m\u001b[38;5;124m'\u001b[39m,TfidfVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))),\n\u001b[0;32m      4\u001b[0m                     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m,LogisticRegression(C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.8\u001b[39m,solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m'\u001b[39m,max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m))\n\u001b[0;32m      5\u001b[0m                 ]\n\u001b[0;32m      6\u001b[0m                 )\n\u001b[0;32m      7\u001b[0m idf_pip\u001b[38;5;241m.\u001b[39mfit(X_train,y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "idf_pip=Pipeline(\n",
    "                [\n",
    "                    ('tf_idf',TfidfVectorizer(ngram_range=(1,1))),\n",
    "                    ('model',LogisticRegression(C=.8,solver='sag',max_iter=1000))\n",
    "                ]\n",
    "                )\n",
    "idf_pip.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    "#### Conclusion [‚èÆÔ∏è](#4)[üëÜüèΩ](#0)[‚è≠Ô∏è](#6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"6\"></a> \n",
    "#### <i>References</i> [‚èÆÔ∏è](#5)[üëÜüèΩ](#0)[‚è≠Ô∏è](#7)\n",
    "Here is some text with a reference to the [Python documentation](https://docs.python.org/).\n",
    "\n",
    "...\n",
    "\n",
    "Here are some references for more information on the libraries used:\n",
    "\n",
    "- [Pandas documentation](https://pandas.pydata.org/docs/)\n",
    "- [NumPy documentation](https://numpy.org/doc/stable/)\n",
    "- [Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "\n",
    "üçÄ Auteurs\n",
    "- üßëüèæ‚Äçüíª Kikia DIA\n",
    "- üßëüèæ‚Äçüíª Mouhamadou Naby DIA\n",
    "- üßëüèæ‚Äçüíª Ndeye Awa SALANE\n",
    "\n",
    "üçÄ Affiliations\n",
    "- üéì Ecole Polytechnique de THIES\n",
    "\n",
    "üçÄ D√©partement \n",
    "- üíª G√©nie Informatique et T√©l√©communications\n",
    "\n",
    "üçÄ Niveau\n",
    "- üìö DIC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
